{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import math\n",
    "import copy\n",
    "import pickle\n",
    "import zipfile\n",
    "from textwrap import wrap\n",
    "from pathlib import Path\n",
    "from itertools import zip_longest\n",
    "from collections import defaultdict\n",
    "from urllib.error import URLError\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F \n",
    "from torch.optim.lr_scheduler import _LRScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below ensures that we seed all random generators with the same value to get reproducible results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def set_random_seed(state=1):\n",
    "    gens = (np.random.seed, torch.manual_seed, torch.cuda.manual_seed)\n",
    "    for set_state in gens:\n",
    "        set_state(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "RANDOM_STATE = 1\n",
    "set_random_seed(RANDOM_STATE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "c:\\users\\lihaiyu\\env\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3444: FutureWarning: Could not cast to uint32, falling back to object. This behavior is deprecated. In a future version, when a dtype is passed to 'DataFrame', either all columns will be cast to that dtype, or a TypeError will be raised\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "ROOT_DIR = os.getcwd()\n",
    "DATA_DIR = os.path.join(ROOT_DIR, 'ml-1m/')\n",
    "ratings_list = [i.strip().split(\"::\") for i in open(os.path.join(DATA_DIR,'ratings.dat'), 'r').readlines()]\n",
    "users_list = [i.strip().split(\"::\") for i in open(os.path.join(DATA_DIR,'users.dat'), 'r').readlines()]\n",
    "movies_list = [i.strip().split(\"::\") for i in open(os.path.join(DATA_DIR,'movies.dat'),encoding='latin-1').readlines()]\n",
    "ratings = pd.DataFrame(ratings_list, columns = ['userId', 'movieId', 'rating', 'timestamp'], dtype = np.uint32)\n",
    "movies = pd.DataFrame(movies_list, columns = ['movieId', 'title', 'genres'])\n",
    "movies['movieId'] = movies['movieId'].apply(pd.to_numeric)\n",
    "ratings['rating'] = ratings['rating'].apply(pd.to_numeric)\n",
    "ratings['userId'] = ratings['userId'].apply(pd.to_numeric)\n",
    "users_df = pd.DataFrame(users_list, columns=['userId','gender','age','occupation','zip-code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "   userId movieId  rating  timestamp\n0       1    1193       5  978300760\n1       1     661       3  978302109\n2       1     914       3  978301968\n3       1    3408       4  978300275\n4       1    2355       5  978824291",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>userId</th>\n      <th>movieId</th>\n      <th>rating</th>\n      <th>timestamp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1193</td>\n      <td>5</td>\n      <td>978300760</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>661</td>\n      <td>3</td>\n      <td>978302109</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>914</td>\n      <td>3</td>\n      <td>978301968</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>3408</td>\n      <td>4</td>\n      <td>978300275</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>2355</td>\n      <td>5</td>\n      <td>978824291</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 15
    }
   ],
   "source": [
    "ratings.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "   movieId                               title                        genres\n0        1                    Toy Story (1995)   Animation|Children's|Comedy\n1        2                      Jumanji (1995)  Adventure|Children's|Fantasy\n2        3             Grumpier Old Men (1995)                Comedy|Romance\n3        4            Waiting to Exhale (1995)                  Comedy|Drama\n4        5  Father of the Bride Part II (1995)                        Comedy",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>movieId</th>\n      <th>title</th>\n      <th>genres</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>Toy Story (1995)</td>\n      <td>Animation|Children's|Comedy</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>Jumanji (1995)</td>\n      <td>Adventure|Children's|Fantasy</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>Grumpier Old Men (1995)</td>\n      <td>Comedy|Romance</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>Waiting to Exhale (1995)</td>\n      <td>Comedy|Drama</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>Father of the Bride Part II (1995)</td>\n      <td>Comedy</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 10
    }
   ],
   "source": [
    "movies.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Dataset Preview\n",
    "\n",
    "First of all, let's repeat the steps of the lecture where the dataset is loaded and prepared for further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def tabular_preview(ratings, n=15):\n",
    "    \"\"\"Creates a cross-tabular view of users vs movies.\"\"\"\n",
    "    \n",
    "    user_groups = ratings.groupby('userId')['rating'].count()\n",
    "    top_users = user_groups.sort_values(ascending=False)[:15]\n",
    "\n",
    "    movie_groups = ratings.groupby('movieId')['rating'].count()\n",
    "    top_movies = movie_groups.sort_values(ascending=False)[:15]\n",
    "\n",
    "    top = (\n",
    "        ratings.\n",
    "        join(top_users, rsuffix='_r', how='inner', on='userId').\n",
    "        join(top_movies, rsuffix='_r', how='inner', on='movieId'))\n",
    "\n",
    "    return pd.crosstab(top.userId, top.movieId, top.rating, aggfunc=np.sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "movieId  110  1196  1198  1210  1270  1580  2028  2571  260  2762  2858  480  \\\nuserId                                                                         \n889      4.0   4.0   NaN   3.0   4.0   3.0   3.0   5.0  4.0   NaN   2.0  3.0   \n1015     4.0   4.0   5.0   4.0   4.0   4.0   5.0   5.0  5.0   5.0   4.0  4.0   \n1150     2.0   4.0   2.0   3.0   2.0   2.0   2.0   1.0  5.0   2.0   4.0  NaN   \n1181     3.0   4.0   3.0   3.0   3.0   4.0   4.0   5.0  4.0   4.0   3.0  2.0   \n1449     3.0   3.0   4.0   2.0   2.0   4.0   3.0   4.0  3.0   4.0   4.0  2.0   \n1680     1.0   5.0   5.0   3.0   3.0   4.0   5.0   3.0  2.0   5.0   5.0  5.0   \n1941     5.0   5.0   5.0   5.0   5.0   5.0   5.0   3.0  5.0   5.0   1.0  5.0   \n1980     4.0   4.0   5.0   4.0   5.0   4.0   5.0   5.0  4.0   5.0   5.0  4.0   \n2063     5.0   4.0   4.0   4.0   4.0   3.0   2.0   5.0  4.0   4.0   5.0  4.0   \n2909     5.0   5.0   5.0   5.0   5.0   5.0   5.0   4.0  5.0   5.0   5.0  5.0   \n3618     3.0   3.0   4.0   3.0   4.0   3.0   3.0   3.0  4.0   4.0   4.0  4.0   \n4169     4.0   5.0   5.0   5.0   4.0   4.0   5.0   4.0  5.0   5.0   5.0  5.0   \n4277     5.0   5.0   5.0   4.0   5.0   4.0   5.0   5.0  5.0   5.0   5.0  5.0   \n4344     5.0   5.0   5.0   5.0   4.0   3.0   2.0   5.0  5.0   5.0   5.0  4.0   \n5795     5.0   5.0   5.0   4.0   5.0   1.0   5.0   1.0  5.0   2.0   5.0  5.0   \n\nmovieId  589  593  608  \nuserId                  \n889      5.0  5.0  4.0  \n1015     5.0  5.0  5.0  \n1150     2.0  3.0  5.0  \n1181     5.0  3.0  3.0  \n1449     2.0  5.0  5.0  \n1680     5.0  5.0  5.0  \n1941     3.0  5.0  4.0  \n1980     4.0  5.0  5.0  \n2063     2.0  5.0  2.0  \n2909     4.0  5.0  5.0  \n3618     3.0  5.0  4.0  \n4169     4.0  5.0  5.0  \n4277     5.0  5.0  5.0  \n4344     4.0  2.0  5.0  \n5795     4.0  5.0  4.0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>movieId</th>\n      <th>110</th>\n      <th>1196</th>\n      <th>1198</th>\n      <th>1210</th>\n      <th>1270</th>\n      <th>1580</th>\n      <th>2028</th>\n      <th>2571</th>\n      <th>260</th>\n      <th>2762</th>\n      <th>2858</th>\n      <th>480</th>\n      <th>589</th>\n      <th>593</th>\n      <th>608</th>\n    </tr>\n    <tr>\n      <th>userId</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>889</th>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>NaN</td>\n      <td>3.0</td>\n      <td>4.0</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>5.0</td>\n      <td>4.0</td>\n      <td>NaN</td>\n      <td>2.0</td>\n      <td>3.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>1015</th>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>5.0</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>1150</th>\n      <td>2.0</td>\n      <td>4.0</td>\n      <td>2.0</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>5.0</td>\n      <td>2.0</td>\n      <td>4.0</td>\n      <td>NaN</td>\n      <td>2.0</td>\n      <td>3.0</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>1181</th>\n      <td>3.0</td>\n      <td>4.0</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>5.0</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>5.0</td>\n      <td>3.0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>1449</th>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>4.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>4.0</td>\n      <td>3.0</td>\n      <td>4.0</td>\n      <td>3.0</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>1680</th>\n      <td>1.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>4.0</td>\n      <td>5.0</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>1941</th>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>3.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>1.0</td>\n      <td>5.0</td>\n      <td>3.0</td>\n      <td>5.0</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>1980</th>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>5.0</td>\n      <td>4.0</td>\n      <td>5.0</td>\n      <td>4.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>4.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>2063</th>\n      <td>5.0</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>5.0</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>5.0</td>\n      <td>4.0</td>\n      <td>2.0</td>\n      <td>5.0</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>2909</th>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>4.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>4.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>3618</th>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>4.0</td>\n      <td>3.0</td>\n      <td>4.0</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>3.0</td>\n      <td>5.0</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>4169</th>\n      <td>4.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>5.0</td>\n      <td>4.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>4.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>4277</th>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>4.0</td>\n      <td>5.0</td>\n      <td>4.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>4344</th>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>4.0</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>2.0</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>5795</th>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>4.0</td>\n      <td>5.0</td>\n      <td>1.0</td>\n      <td>5.0</td>\n      <td>1.0</td>\n      <td>5.0</td>\n      <td>2.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>4.0</td>\n      <td>5.0</td>\n      <td>4.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 12
    }
   ],
   "source": [
    "tabular_preview(ratings, movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Dataset Preparation\n",
    "\n",
    "Next we need a couple of helpers to prepare the data and make it ready for the training loop. The first one was already implemented in the original post:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def create_dataset(ratings, top=None):\n",
    "    if top is not None:\n",
    "        ratings.groupby('userId')['rating'].count()\n",
    "    \n",
    "    unique_users = ratings.userId.unique()\n",
    "    user_to_index = {old: new for new, old in enumerate(unique_users)}\n",
    "    new_users = ratings.userId.map(user_to_index)\n",
    "    \n",
    "    unique_movies = ratings.movieId.unique()\n",
    "    movie_to_index = {old: new for new, old in enumerate(unique_movies)}\n",
    "    new_movies = ratings.movieId.map(movie_to_index)\n",
    "    \n",
    "    n_users = unique_users.shape[0]\n",
    "    n_movies = unique_movies.shape[0]\n",
    "    \n",
    "    X = pd.DataFrame({'user_id': new_users, 'movie_id': new_movies})\n",
    "    y = ratings['rating'].astype(np.float32)\n",
    "    return (n_users, n_movies), (X, y), (user_to_index, movie_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Embeddings: 6040 users, 3706 movies\n",
      "Dataset shape: (1000209, 2)\n",
      "Target shape: (1000209,)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "(n, m), (X, y), _ = create_dataset(ratings)\n",
    "print(f'Embeddings: {n} users, {m} movies')\n",
    "print(f'Dataset shape: {X.shape}')\n",
    "print(f'Target shape: {y.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second one is a small class that allows to iterate through the dataset one batch after another:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class ReviewsIterator:\n",
    "    \n",
    "    def __init__(self, X, y, batch_size=32, shuffle=True):\n",
    "        X, y = np.asarray(X), np.asarray(y)\n",
    "        \n",
    "        if shuffle:\n",
    "            index = np.random.permutation(X.shape[0])\n",
    "            X, y = X[index], y[index]\n",
    "            \n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.n_batches = int(math.ceil(X.shape[0] // batch_size))\n",
    "        self._current = 0\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "    \n",
    "    def next(self):\n",
    "        if self._current >= self.n_batches:\n",
    "            raise StopIteration()\n",
    "        k = self._current\n",
    "        self._current += 1\n",
    "        bs = self.batch_size\n",
    "        return self.X[k*bs:(k + 1)*bs], self.y[k*bs:(k + 1)*bs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A convenice function `batches` allows us to split the dataset into smaller chunks during training/validation process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def batches(X, y, bs=32, shuffle=True):\n",
    "    for xb, yb in ReviewsIterator(X, y, bs, shuffle):\n",
    "        xb = torch.LongTensor(xb)\n",
    "        yb = torch.FloatTensor(yb)\n",
    "        yield xb, yb.view(-1, 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "for x_batch, y_batch in batches(X, y, bs=4):\n",
    "    print(x_batch)\n",
    "    print(y_batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Embeddings\n",
    "\n",
    "In this secion we're going to create an `nn.Module` for our embedding network. The main difference from the class provided in the original post is that this class is a bit more generic: it allows to create a network of arbirary depth, with or without dropouts. Also, the `forward` method accepts an additional `minmax` argument which is expected to be a tuple with minimum and maximum values of ratings from the dataset to normalize the predicted value into specific range.\n",
    "\n",
    "Note that to create a group of hidden layers we use a [generator](https://wiki.python.org/moin/Generators) function `gen_layers` that yields linear and dropout layers depending on values from `hidden` and `dropouts` arrays. Also note that we use the `nn.Sequence` class to group hidden layers into a single module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class EmbeddingNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Creates a dense network with embedding layers.\n",
    "    \n",
    "    Args:\n",
    "    \n",
    "        n_users:            \n",
    "            Number of unique users in the dataset.\n",
    "\n",
    "        n_movies: \n",
    "            Number of unique movies in the dataset.\n",
    "\n",
    "        n_factors: \n",
    "            Number of columns in the embeddings matrix.\n",
    "\n",
    "        embedding_dropout: \n",
    "            Dropout rate to apply right after embeddings layer.\n",
    "\n",
    "        hidden:\n",
    "            A single integer or a list of integers defining the number of \n",
    "            units in hidden layer(s).\n",
    "\n",
    "        dropouts: \n",
    "            A single integer or a list of integers defining the dropout \n",
    "            layers rates applyied right after each of hidden layers.\n",
    "            \n",
    "    \"\"\"\n",
    "    def __init__(self, n_users, n_movies,\n",
    "                 n_factors=50, embedding_dropout=0.02, \n",
    "                 hidden=10, dropouts=0.2):\n",
    "        \n",
    "        super().__init__()\n",
    "        hidden = get_list(hidden)\n",
    "        dropouts = get_list(dropouts)\n",
    "        n_last = hidden[-1]\n",
    "        \n",
    "        def gen_layers(n_in):\n",
    "            \"\"\"\n",
    "            A generator that yields a sequence of hidden layers and \n",
    "            their activations/dropouts.\n",
    "            \n",
    "            Note that the function captures `hidden` and `dropouts` \n",
    "            values from the outer scope.\n",
    "            \"\"\"\n",
    "            nonlocal hidden, dropouts\n",
    "            assert len(dropouts) <= len(hidden)\n",
    "            \n",
    "            for n_out, rate in zip_longest(hidden, dropouts):\n",
    "                yield nn.Linear(n_in, n_out)\n",
    "                yield nn.ReLU()\n",
    "                if rate is not None and rate > 0.:\n",
    "                    yield nn.Dropout(rate)\n",
    "                n_in = n_out\n",
    "            \n",
    "        self.u = nn.Embedding(n_users, n_factors)\n",
    "        self.m = nn.Embedding(n_movies, n_factors)\n",
    "        self.drop = nn.Dropout(embedding_dropout)\n",
    "        self.hidden = nn.Sequential(*list(gen_layers(n_factors * 2)))\n",
    "        self.fc = nn.Linear(n_last, 1)\n",
    "        self._init()\n",
    "        \n",
    "    def forward(self, users, movies, minmax=None):\n",
    "        print(self.u(users))\n",
    "        print(self.m(users))\n",
    "        features = torch.cat([self.u(users), self.m(movies)], dim=1)\n",
    "        x = self.drop(features)\n",
    "        x = self.hidden(x)\n",
    "        out = torch.sigmoid(self.fc(x))\n",
    "        if minmax is not None:\n",
    "            min_rating, max_rating = minmax\n",
    "            out = out*(max_rating - min_rating + 1) + min_rating - 0.5\n",
    "        return out\n",
    "    \n",
    "    def _init(self):\n",
    "        \"\"\"\n",
    "        Setup embeddings and hidden layers with reasonable initial values.\n",
    "        \"\"\"\n",
    "        \n",
    "        def init(m):\n",
    "            if type(m) == nn.Linear:\n",
    "                torch.nn.init.xavier_uniform_(m.weight)\n",
    "                m.bias.data.fill_(0.01)\n",
    "                \n",
    "        self.u.weight.data.uniform_(-0.05, 0.05)\n",
    "        self.m.weight.data.uniform_(-0.05, 0.05)\n",
    "        self.hidden.apply(init)\n",
    "        init(self.fc)\n",
    "    \n",
    "    \n",
    "def get_list(n):\n",
    "    if isinstance(n, (int, float)):\n",
    "        return [n]\n",
    "    elif hasattr(n, '__iter__'):\n",
    "        return list(n)\n",
    "    raise TypeError('layers configuraiton should be a single number or a list of numbers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, if we want to create a network with a single hidden layer and dropout, we can use the following code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "EmbeddingNet(n, m, n_factors=100, hidden=100, dropouts=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If case if something more involved is required, pass a list of values instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "EmbeddingNet(n, m, n_factors=100, hidden=[100, 200, 300], dropouts=[0.25, 0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cyclical Learning Rate (CLR)\n",
    "\n",
    "One of the `fastai` library features is the cyclical learning rate scheduler. We can implement something similar inheriting the `_LRScheduler` class from the `torch` library. Following the [original paper's](https://arxiv.org/abs/1506.01186) pseudocode, this [CLR Keras callback implementation](https://github.com/bckenstler/CLR), and making a couple of adjustments to support [cosine annealing](https://pytorch.org/docs/stable/optim.html#torch.optim.lr_scheduler.CosineAnnealingLR) with restarts, let's create our own CLR scheduler.\n",
    "\n",
    "The implementation of this idea is quite simple. The [base PyTorch scheduler class](https://pytorch.org/docs/stable/_modules/torch/optim/lr_scheduler.html) has the `get_lr()` method that is invoked each time when we call the `step()` method. The method should return a list of learning rates depending on the current training epoch. In our case, we have the same learning rate for all of the layers, and therefore, we return a list with a single value. \n",
    "\n",
    "The next cell defines a `CyclicLR` class that expectes a single callback function. This function should accept the current training epoch and the base value of learning rate, and return a new learning rate value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class CyclicLR(_LRScheduler):\n",
    "    \n",
    "    def __init__(self, optimizer, schedule, last_epoch=-1):\n",
    "        assert callable(schedule)\n",
    "        self.schedule = schedule\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        return [self.schedule(self.last_epoch, lr) for lr in self.base_lrs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our scheduler is very similar to [LambdaLR](https://pytorch.org/docs/stable/optim.html#torch.optim.lr_scheduler.LambdaLR) one but expects a bit different callback signature. \n",
    "\n",
    "So now we only need to define appropriate scheduling functions. We're createing a couple of functions that accept scheduling parameters and return a _new function_ with the appropriate signature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def triangular(step_size, max_lr, method='triangular', gamma=0.99):\n",
    "    \n",
    "    def scheduler(epoch, base_lr):\n",
    "        period = 2 * step_size\n",
    "        cycle = math.floor(1 + epoch/period)\n",
    "        x = abs(epoch/step_size - 2*cycle + 1)\n",
    "        delta = (max_lr - base_lr)*max(0, (1 - x))\n",
    "\n",
    "        if method == 'triangular':\n",
    "            pass  # we've already done\n",
    "        elif method == 'triangular2':\n",
    "            delta /= float(2 ** (cycle - 1))\n",
    "        elif method == 'exp_range':\n",
    "            delta *= (gamma**epoch)\n",
    "        else:\n",
    "            raise ValueError('unexpected method: %s' % method)\n",
    "            \n",
    "        return base_lr + delta\n",
    "        \n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def cosine(t_max, eta_min=0):\n",
    "    \n",
    "    def scheduler(epoch, base_lr):\n",
    "        t = epoch % t_max\n",
    "        return eta_min + (base_lr - eta_min)*(1 + math.cos(math.pi*t/t_max))/2\n",
    "    \n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand how the created functions work, and to check the correctness of our implementation, let's create a couple of plots visualizing learning rates changes depending on the number of epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_lr(schedule):\n",
    "    ts = list(range(1000))\n",
    "    y = [schedule(t, 0.001) for t in ts]\n",
    "    plt.plot(ts, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plot_lr(triangular(250, 0.005))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plot_lr(triangular(250, 0.005, 'triangular2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plot_lr(triangular(250, 0.005, 'exp_range', gamma=0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plot_lr(cosine(t_max=500, eta_min=0.0005))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that cosine annealing scheduler is a bit different from other schedules as soon as it starts with `base_lr` and gradually decreases it to the minimal value while triangle schedulers increase the original rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training Loop\n",
    "\n",
    "Now we're ready to start the training process. First of all, let's split the original dataset using [train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function from the `scikit-learn` library. (Though you can use anything else instead, like, [get_cv_idxs](https://github.com/fastai/fastai/blob/921777feb46f215ed2b5f5dcfcf3e6edd299ea92/fastai/dataset.py#L6-L22) from `fastai`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE)\n",
    "datasets = {'train': (X_train, y_train), 'val': (X_valid, y_valid)}\n",
    "dataset_sizes = {'train': len(X_train), 'val': len(X_valid)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "minmax = ratings.rating.min().astype(float), ratings.rating.max().astype(float)\n",
    "minmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "net = EmbeddingNet(\n",
    "    n_users=n, n_movies=m, \n",
    "    n_factors=100, hidden=[100, 200, 300], \n",
    "    embedding_dropout=0.05, dropouts=[0.5, 0.5, 0.25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell is preparing and running the training loop with cyclical learning rate, validation and early stopping. We use `Adam` optimizer with cosine-annealing learnign rate. The rate is decreased on each batch during `2` epochs, and then is reset to the original value.\n",
    "\n",
    "Note that our loop has two phases. One of them is called `train`. During this phase, we update our network's weights and change the learning rate. The another one is called `val` and is used to check the model's performence. When the loss value decreases, we save model parameters to restore them later. If there is no improvements after `10` sequential training epochs, we exit from the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "wd = 1e-5\n",
    "bs = 2000\n",
    "n_epochs = 100\n",
    "patience = 10\n",
    "no_improvements = 0\n",
    "best_loss = np.inf\n",
    "best_weights = None\n",
    "history = []\n",
    "lr_history = []\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "net.to(device)\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr, weight_decay=wd)\n",
    "iterations_per_epoch = int(math.ceil(dataset_sizes['train'] // bs))\n",
    "scheduler = CyclicLR(optimizer, cosine(t_max=iterations_per_epoch * 2, eta_min=lr/10))\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    stats = {'epoch': epoch + 1, 'total': n_epochs}\n",
    "    \n",
    "    for phase in ('train', 'val'):\n",
    "        training = phase == 'train'\n",
    "        running_loss = 0.0\n",
    "        n_batches = 0\n",
    "        \n",
    "        for batch in batches(*datasets[phase], shuffle=training, bs=bs):\n",
    "            x_batch, y_batch = [b for b in batch]\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # compute gradients only during 'train' phase\n",
    "            with torch.set_grad_enabled(training):\n",
    "                outputs = net(x_batch[:, 0], x_batch[:, 1], minmax)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                \n",
    "                # don't update weights and rates when in 'val' phase\n",
    "                if training:\n",
    "                    scheduler.step()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    lr_history.extend(scheduler.get_lr())\n",
    "                    \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        epoch_loss = running_loss / dataset_sizes[phase]\n",
    "        stats[phase] = epoch_loss\n",
    "        \n",
    "        # early stopping: save weights of the best model so far\n",
    "        if phase == 'val':\n",
    "            if epoch_loss < best_loss:\n",
    "                print('loss improvement on epoch: %d' % (epoch + 1))\n",
    "                best_loss = epoch_loss\n",
    "                best_weights = copy.deepcopy(net.state_dict())\n",
    "                no_improvements = 0\n",
    "            else:\n",
    "                no_improvements += 1\n",
    "                \n",
    "    history.append(stats)\n",
    "    print('[{epoch:03d}/{total:03d}] train: {train:.4f} - val: {val:.4f}'.format(**stats))\n",
    "    if no_improvements >= patience:\n",
    "        print('early stopping after epoch {epoch:03d}'.format(**stats))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Metrics\n",
    "\n",
    "To visualize the training process and to check the correctness of the learning rate scheduling, let's create a couple of plots using collected stats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "ax = pd.DataFrame(history).drop(columns='total').plot(x='epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "_ = plt.plot(lr_history[:2*iterations_per_epoch])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the learning rate is updated in accordance with cosine annealing schedule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training process was terminated after _16 epochs_. Now we're going to restore the best weights saved during training, and apply the model to the validation subset of the data to see the final model's performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "net.load_state_dict(best_weights)\n",
    "torch.save(\n",
    "            best_weights,\n",
    "            './user_item_embedding.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "groud_truth, predictions = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in batches(*datasets['val'], shuffle=False, bs=bs):\n",
    "        x_batch, y_batch = [b.to(device) for b in batch]\n",
    "        outputs = net(x_batch[:, 0], x_batch[:, 1], minmax)\n",
    "        groud_truth.extend(y_batch.tolist())\n",
    "        predictions.extend(outputs.tolist())\n",
    "\n",
    "groud_truth = np.asarray(groud_truth).ravel()\n",
    "predictions = np.asarray(predictions).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "final_loss = np.sqrt(np.mean((predictions - groud_truth)**2))\n",
    "print(f'Final RMSE: {final_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "with open('best.weights', 'wb') as file:\n",
    "    pickle.dump(best_weights, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Bonus: Embeddings Visualization\n",
    "\n",
    "Finally, we can create a couple of visualizations to show how various movies are encoded in embeddings space. Again, we're repeting the approach shown in the original post and apply the [Principal Components Analysis](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) to reduce the dimentionality of embeddings and show some of them with bar plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading previously saved weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "with open('best.weights', 'rb') as file:\n",
    "    print(file)\n",
    "    best_weights = pickle.load(file)\n",
    "net.load_state_dict(best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def to_numpy(tensor):\n",
    "    return tensor.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the mappings between original users's and movies's IDs, and new contiguous values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "_, _, (user_id_map, movie_id_map) = create_dataset(ratings)\n",
    "embed_to_original = {v: k for k, v in movie_id_map.items()}\n",
    "popular_movies = ratings.groupby('movieId').movieId.count().sort_values(ascending=False).values[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducing the dimensionality of movie embeddings vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "embed = to_numpy(net.m.weight.data)\n",
    "pca = PCA(n_components=5)\n",
    "components = pca.fit(embed[popular_movies].T).components_\n",
    "components.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, creating a joined data frame with projected embeddings and movies they represent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "components_df = pd.DataFrame(components.T, columns=[f'fc{i}' for i in range(pca.n_components_)])\n",
    "movie_ids = [embed_to_original[idx] for idx in components_df.index]\n",
    "meta = movies.set_index('movieId')\n",
    "components_df['movie_id'] = movie_ids\n",
    "components_df['title'] = meta.loc[movie_ids].title.values\n",
    "components_df['genres'] = meta.loc[movie_ids].genres.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "components_df.sample(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_components(components, component, ascending=False):\n",
    "    fig, ax = plt.subplots(figsize=(18, 12))\n",
    "    \n",
    "    subset = components.sort_values(by=component, ascending=ascending).iloc[:12]\n",
    "    columns = components_df.columns\n",
    "    features = columns[columns.str.startswith('fc')].tolist()\n",
    "    \n",
    "    fc = subset[features]\n",
    "    titles = ['\\n'.join(wrap(t, width=10)) for t in subset.title]\n",
    "    genres = subset.genres.str.replace('|', '\\n')\n",
    "    labels = [f'{t}\\n\\n{g}' for t, g in zip(titles, genres)]\n",
    "    \n",
    "    fc.plot(ax=ax, kind='bar')\n",
    "    y_ticks = [f'{t:2.2f}' for t in ax.get_yticks()]\n",
    "    ax.set_xticklabels(labels, rotation=0, fontsize=14)\n",
    "    ax.set_yticklabels(y_ticks, fontsize=14)\n",
    "    ax.legend(loc='best', fontsize=14)\n",
    "    \n",
    "    plot_title = f\"Movies with {['highest', 'lowest'][ascending]} '{component}' component values\" \n",
    "    ax.set_title(plot_title, fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plot_components(components_df, 'fc0', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plot_components(components_df, 'fc0', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the visualizations above, we could make a guess that the `red` component mostly reflects the rate of \"seriousness\" in the movie. Comedies, animations and adventures mostly have the negative values of this feature while more \"dramatical\" movies have high levels of this component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}